# from environment5 import environment5
from read_data_old import read_data
import pdb 
from collections import defaultdict

class utilities: 
    def __init__(self) -> None:
        self.relevant = None

    def get_prior(self, dataset):
        if dataset == 'birdstrikes1':
            self.relevant = ['"dam_eng1"', '"dam_eng2"', '"dam_windshld"', '"dam_wing_rot"', '"damage"', '"ac_class"', '"precip"', '"sky"', '"incident_date"']
        elif dataset == 'weather1':
            self.relevant = ['"heavyfog"', '"mist"', '"drizzle"', '"groundfog"', '"tmax"', '"tmin"', '"tmin_f"', '"tmax_f"', '"highwinds"']
        else: #"FAA"
            self.relevant = ['"cancelled"', '"diverted"', '"arrdelay"', '"depdelay"', '"flightdate"', '"uniquecarrier"', '"distance"']

    #intra = previous interaction
    #intrb = current interaction
    def check_diff(self, intra, intrb):
        #check if 2 interactions are same or not [so whether the user used action: Modify / Keep]
        if sorted(intra) == sorted(intrb): #Both interactions are same
            return False
        else:
            return True #the interactions are different
        
    #calculate reward based on memory
    def get_reward(self, interactions, dataset):
        self.get_prior(dataset)
        mem_rewards = []
        history = defaultdict(int)
        for items in interactions[0]:
            history[items] = 1
        for i in range(1, len(interactions)):
            newly_added = []
            for items in interactions[i]:
                if items not in interactions[i-1]:
                    newly_added.append(items)
            reward = 0
            if len(newly_added) > 0:
                for items in newly_added:
                    if items in self.relevant: #the newly added item is from relevant
                        reward += 0.5
                    elif items in history: # reusing an attribute that has been used in the past, different from reusing all attributes from i-1 interaction
                        reward += 0.25
                    else: # adding / exploring a completely new attribute
                        reward += 1
                    history[items] = 1
            else:
                for items in interactions[i]:
                    if items in history:
                        reward += 0.25 #repeating past interaction 
                    
            mem_rewards.append(reward)
            # print(interactions[i-1])
            # print(newly_added, reward)
            # print(interactions[i])
        return mem_rewards
    
    def generate(self, data, dataset):
        interactions = []
        #Converting string interactions into python lists
        for itrs in data:
            if len(itrs) <= 2:
                continue
            itrs = itrs.strip('[]')
            states = itrs.split(', ')
            if '"number of records"' in states:
                states.remove('"number of records"')
            interactions.append(states)

        mem_states = []
        mem_action = []
        mem_states.append("Hypothesis_Generation") #Hypothesis Generation
        #the action in step i, is generated by observing interaction (i & i+1) [so the last interaction will not have any action]
        for i in range(1, len(interactions)):
            # print(interactions[i-1])
            if self.check_diff(interactions[i - 1], interactions[i]):
                mem_states.append("Hypothesis_Generation")
                mem_action.append("Modify")
                # print("Hypothesis_Generation", "Modify")
            else: 
                mem_states.append("Sensemaking")
                mem_action.append("Keep")
                # print("Sensemaking", "Keep")
            # print(interactions[i])
        mem_rewards = self.get_reward(interactions, dataset)
        # print(len(mem_rewards))
        mem_states.pop(len(mem_states)-1)
        return mem_states, mem_action, mem_rewards