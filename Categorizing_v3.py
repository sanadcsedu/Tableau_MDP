# from environment5 import environment5
from read_data import read_data
import pdb 
from collections import defaultdict
from relevant import rewards_v2

class utilities: 
    def __init__(self) -> None:
        self.relevant = None

    def get_prior(self, dataset):
        if dataset == 'birdstrikes1':
            self.relevant = ['"dam_eng1"', '"dam_eng2"', '"dam_windshld"', '"dam_wing_rot"', '"damage"', '"ac_class"', '"precip"', '"sky"', '"incident_date"']
        elif dataset == 'weather1':
            self.relevant = ['"heavyfog"', '"mist"', '"drizzle"', '"groundfog"', '"tmax"', '"tmin"', '"tmin_f"', '"tmax_f"', '"highwinds"']
        else: #"FAA"
            self.relevant = ['"cancelled"', '"diverted"', '"arrdelay"', '"depdelay"', '"flightdate"', '"uniquecarrier"', '"distance"']

    #intra = previous interaction
    #intrb = current interaction
    def check_diff(self, intra, intrb):
        #check if 2 interactions are same or not [so whether the user used action: Modify / Keep]
        if sorted(intra) == sorted(intrb): #Both interactions are same
            return False
        else:
            return True #the interactions are different
    

    #calculate reward based on memory
    def get_reward(self, interactions, dataset):
        self.get_prior(dataset)
        mem_rewards = []
        history = defaultdict(int)
        for items in interactions[0]:
            history[items] = 1
        for i in range(1, len(interactions)):
            newly_added = []
            for items in interactions[i]:
                if items not in interactions[i-1]:
                    newly_added.append(items)
            reward = 0
            if len(newly_added) > 0:
                for items in newly_added:
                    if items in self.relevant: #the newly added item is from relevant
                        reward += 0.5
                    elif items in history: # reusing an attribute that has been used in the past, different from reusing all attributes from i-1 interaction
                        reward += 0.25
                    else: # adding / exploring a completely new attribute
                        reward += 1
                    history[items] = 1
            else:
                for items in interactions[i]:
                    if items in history:
                        reward += 0.25 #repeating past interaction 
                    
            mem_rewards.append(reward)
            # print(interactions[i-1])
            # print(newly_added, reward)
            # print(interactions[i])
        return mem_rewards
    
    def generate(self, data, dataset):
        interactions = []
        #Converting string interactions into python lists
        for itrs in data:
            # if len(itrs) <= 2:
            #     continue
            itrs = itrs.strip('[]')
            states = itrs.split(', ')
            if '"number of records"' in states:
                states.remove('"number of records"')
            interactions.append(states)

        mem_states = []
        mem_action = []
        mem_rewards = []
        mem_interactions = []
        for_reward = rewards_v2()
        self.get_prior(dataset)
        history = defaultdict(int)
        for items in interactions[0]:
            history[items] = 1
        mem_interactions.append(interactions[0])
        mem_states.append("Foraging") #Hypothesis Generation
        #the action in step i, is generated by observing interaction (i & i+1) [so the last interaction will not have any action]
        for i in range(1, len(interactions)):
            # print(interactions[i-1])
            if self.check_diff(interactions[i - 1], interactions[i]):
                #check if any attribute was dropped
                flag = False
                for items in interactions[i-1]:
                    if items not in interactions[i]:
                        flag = True
                        break
                if flag and interactions[i-1] != ['']: #an attribute was dropped
                    mem_interactions.append(interactions[i])
                    mem_states.append("Navigation")
                    mem_action.append("Remove")
                    mem_rewards.append(0.2)
                    # print("---", mem_states[len(mem_states) - 1], mem_action[len(mem_action) - 1], mem_rewards[len(mem_rewards) - 1])
            
                #check if any new attribute was explored
                flag = False
                for items in interactions[i]:
                    if items not in interactions[i-1]:
                        flag = True
                        break
                if flag and interactions[i] != ['']:
                    mem_interactions.append(interactions[i])
                    mem_states.append("Foraging")
                    mem_action.append("Add")
                    #Calculating reward for Foraging state
                    reward = 0
                    for items in interactions[i]:
                        if items in history:
                            reward += for_reward.get_reward(dataset, items)
                    mem_rewards.append(reward)
                    # print("---", mem_states[len(mem_states) - 1], mem_action[len(mem_action) - 1], mem_rewards[len(mem_rewards) - 1])
            else: 
                mem_interactions.append(interactions[i])
                mem_states.append("Sensemaking")
                mem_action.append("Keep")
                #Calculating reward for Sensemaking Phase
                reward = 0
                for items in interactions[i]:
                    if items in history:
                        reward += for_reward.get_reward(dataset, items) #repeating past interaction 
                mem_rewards.append(reward)
            #     print("---", mem_states[len(mem_states) - 1], mem_action[len(mem_action) - 1], mem_rewards[len(mem_rewards) - 1])
            # print(interactions[i])

        # mem_rewards = self.get_reward(interactions, dataset)
        # print(len(mem_rewards))
        mem_states.pop(len(mem_states)-1)
        return mem_interactions, mem_states, mem_action, mem_rewards
    
if __name__ == "__main__":
    obj = read_data()
    obj.create_connection(r"Tableau.db")
    data = obj.merge2('birdstrikes1', 73)
    u = utilities()
    raw_states, raw_actions, mem_reward = u.generate(data, 'birdstrikes1')
    print(len(raw_states), len(raw_actions), len(mem_reward))
    # for i in range(len(raw_states)):
    #     print(raw_states[i], raw_actions[i], mem_reward[i])